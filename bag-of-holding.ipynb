{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bag of holding (aka. quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "device = 'cuda' if t.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 640\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pad x to [..., 32, 32] because tritron's tl.dot makes life hard if it doesn't like the shape >_<'\n",
    "        x = nn.functional.pad(x, (2, 2, 2, 2))\n",
    "        return self.mlp(x.flatten(start_dim=-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.no_grad()\n",
    "def eval(model, dataloader=test_loader):\n",
    "    model.eval()\n",
    "    loss, accuracy, count = 0, 0, 0\n",
    "    for x, y in dataloader:\n",
    "        logits = model(x.to(device))\n",
    "        loss += F.cross_entropy(logits, y.to(device)).item()\n",
    "        accuracy += (logits.argmax(1) == y.to(device)).sum().item()\n",
    "        count += len(x)\n",
    "    model.train()\n",
    "    return loss / count, accuracy / count\n",
    "\n",
    "def eval_dict(model, dataloader=test_loader, name='test'):\n",
    "    loss, accuracy = eval(model, dataloader=dataloader)\n",
    "    return {f'{name}_loss': loss, f'{name}_accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, opt, dataloader=train_loader, wnb=True, epochs=1000):\n",
    "    model.train()\n",
    "    if wnb: wandb.init()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for x, y in dataloader:\n",
    "            logits = model(x.to(device))\n",
    "            loss = F.cross_entropy(logits, y.to(device))\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            if wnb: wandb.log({'epoch': epoch} | eval_dict(model) | eval_dict(model, dataloader=train_loader, name='train'))\n",
    "    if wnb: wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = Mnist().to(device)\n",
    "opt = optim.AdamW(mnist.parameters(), lr=3e-4)\n",
    "train(mnist, opt, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(mnist.state_dict(), f'weights/mnist-1024-128-64-10_{datetime.now().strftime(\"%Y-%m-%d_%Hh%M\")}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tiled triton int8 matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atol: 0.0\n",
      "diff > 1e-5: 0 / 1048576\n",
      "allclose: True\n"
     ]
    }
   ],
   "source": [
    "# heavily inspired by: https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html\n",
    "\n",
    "@triton.jit\n",
    "def matmul_kernel(\n",
    "        # Pointers to matrices\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        # Matrix dimensions\n",
    "        M, N, K,\n",
    "        # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "        # by to get the element one row down (A has M rows).\n",
    "        stride_am, stride_ak,\n",
    "        stride_bk, stride_bn,\n",
    "        stride_cm, stride_cn,\n",
    "        # Meta-parameters\n",
    "        BLOCK_SIZE_M: tl.constexpr,\n",
    "        BLOCK_SIZE_N: tl.constexpr,\n",
    "        BLOCK_SIZE_K: tl.constexpr,\n",
    "        GROUP_SIZE_M: tl.constexpr,\n",
    "):\n",
    "    ''' Kernel for computing the matmul C = A x B.\n",
    "    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n",
    "    '''\n",
    "    # -----------------------------------------------------------\n",
    "    # Map program ids `pid` to the block of C it should compute.\n",
    "    # This is done in a grouped ordering to promote L2 data reuse.\n",
    "    # See above `L2 Cache Optimizations` section for details.\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Create pointers for the first blocks of A and B.\n",
    "    # We will advance this pointer as we move in the K direction\n",
    "    # and accumulate\n",
    "    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n",
    "    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n",
    "    # See above `Pointer Arithmetic` section for details\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator = tl.dot(a, b, accumulator)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "    c = accumulator.to(tl.int32)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "def matmul_i8i32(a, b):\n",
    "    ''' matmul for int8 with int32 accumulators '''\n",
    "    assert a.shape[1] == b.shape[0], 'incompatible dimensions'\n",
    "    assert a.is_contiguous(), 'matrix A must be contiguous'\n",
    "    M, K = a.shape\n",
    "    K, N = b.shape\n",
    "    c = t.empty((M, N), device=a.device, dtype=t.int32)\n",
    "    # launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    matmul_kernel[grid](\n",
    "        a, b, c,\n",
    "        M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c.stride(0), c.stride(1),\n",
    "        BLOCK_SIZE_M=32,\n",
    "        BLOCK_SIZE_N=32,\n",
    "        BLOCK_SIZE_K=32,\n",
    "        GROUP_SIZE_M=8,\n",
    "    )\n",
    "    return c\n",
    "\n",
    "a = t.randint(-128, 128, (512, 1024), device='cuda', dtype=t.int8)\n",
    "b = t.randint(-128, 128, (1024, 2048), device='cuda', dtype=t.int8)\n",
    "triton_output = matmul_i8i32(a, b)\n",
    "torch_output = t.matmul(a.to(t.float32), b.to(t.float32))\n",
    "\n",
    "print(f'atol: {t.max(t.abs(triton_output - torch_output))}')\n",
    "print(f'diff > 1e-5: {t.sum(t.abs(triton_output - torch_output) > 1e-5)} / {triton_output.numel()}')\n",
    "print(f'allclose: {triton_output.float().allclose(torch_output)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alternative kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atol: 0.0\n",
      "diff > 1e-5: 0 / 1048576\n",
      "allclose: True\n"
     ]
    }
   ],
   "source": [
    "from torch._higher_order_ops.out_dtype import out_dtype\n",
    "\n",
    "a = t.randint(-128, 128, (512, 1024), device='cuda', dtype=t.int8)\n",
    "b = t.randint(-128, 128, (1024, 2048), device='cuda', dtype=t.int8)\n",
    "triton_output = out_dtype(t.ops.aten.mm.default, t.int32, a, b)\n",
    "torch_output = t.matmul(a.to(t.float32), b.to(t.float32))\n",
    "\n",
    "print(f'atol: {t.max(t.abs(triton_output - torch_output))}')\n",
    "print(f'diff > 1e-5: {t.sum(t.abs(triton_output - torch_output) > 1e-5)} / {triton_output.numel()}')\n",
    "print(f'allclose: {triton_output.float().allclose(torch_output)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## symmetric quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squantize(weights, bits=8):\n",
    "    assert bits <= 8 # keep my life simple\n",
    "    maxi = weights.abs().max()\n",
    "    q_maxi = 2 ** (bits - 1) - 1\n",
    "    scale = maxi / q_maxi\n",
    "    quantized_weights = t.clamp(t.round(weights / scale), -q_maxi, q_maxi).to(t.int8)\n",
    "    return quantized_weights, scale\n",
    "\n",
    "def sunquantize(quantized_weights, scale):\n",
    "    return quantized_weights * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_round_trip(weights, bits=8, threshold=0.02):\n",
    "    ''' this is flaky at best '''\n",
    "    qw, scale = squantize(weights, bits=bits)\n",
    "    w = sunquantize(qw, scale)\n",
    "    assert weights.allclose(w, atol=threshold), f'{(weights - w).abs().max()=}'\n",
    "\n",
    "def test_more_bits_do_better(weights):\n",
    "    ''' flaky too '''\n",
    "    def encode_decode_max_abs_error(bits):\n",
    "        qw, scale = squantize(weights, bits=bits)\n",
    "        w = sunquantize(qw, scale)\n",
    "        return (weights - w).abs().max()\n",
    "    error1 = encode_decode_max_abs_error(bits=4)\n",
    "    error2 = encode_decode_max_abs_error(bits=6)\n",
    "    error3 = encode_decode_max_abs_error(bits=8)\n",
    "    assert error1 > error2 > error3, f'{error1=}, {error2=}, {error3=}'\n",
    "\n",
    "for _ in range(100):\n",
    "    xs = t.randn(100, 100)\n",
    "    test_round_trip(xs)\n",
    "    test_more_bits_do_better(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5008046627044678 0.5007808804512024\n",
      "max: 0.9999886751174927 0.9999886155128479\n",
      "min: 0.00014400482177734375 0.0\n",
      "atol: 0.003935694694519043\n",
      "mse: 5.147882347955601e-06\n"
     ]
    }
   ],
   "source": [
    "def squantization_stats(weights):\n",
    "    qw, scale = squantize(weights, bits=8)\n",
    "    w = sunquantize(qw, scale)\n",
    "    print(f'mean: {weights.mean()} {w.mean()}')\n",
    "    print(f'max: {weights.max()} {w.max()}')\n",
    "    print(f'min: {weights.min()} {w.min()}')\n",
    "    print(f'atol: {(weights - w).abs().max()}')\n",
    "    print(f'mse: {(weights - w).pow(2).mean()}')\n",
    "\n",
    "xs = t.rand(10000)\n",
    "squantization_stats(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "x &= x_{quant} * scale_x \\\\\n",
    "w &= w_{quant} * scale_w \\\\\n",
    "y &= x @ w \\\\\n",
    "  &= (x_{quant} * scale_x) @ (w_{quant} * scale_w) \\\\\n",
    "  &= scale_x * scale_w * x_{quant} @ w_{quant}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: -0.08168823271989822 -0.08109944313764572\n",
      "max: 37.009124755859375 36.74298858642578\n",
      "min: -35.65652084350586 -35.63026809692383\n",
      "atol: 0.5931930541992188\n",
      "mse: 0.017821861431002617\n"
     ]
    }
   ],
   "source": [
    "def squantization_matmul_stats(x, w):\n",
    "    y = x @ w\n",
    "    xq, scale_x = squantize(x)\n",
    "    wq, scale_w = squantize(w)\n",
    "    yq = matmul_i8i32(xq, wq)\n",
    "    scale_y = scale_x * scale_w\n",
    "    yy = sunquantize(yq, scale_y)\n",
    "    print(f'mean: {y.mean()} {yy.mean()}')\n",
    "    print(f'max: {y.max()} {yy.max()}')\n",
    "    print(f'min: {y.min()} {yy.min()}')\n",
    "    print(f'atol: {(y - yy).abs().max()}')\n",
    "    print(f'mse: {(y - yy).pow(2).mean()}')\n",
    "    return y, yy\n",
    "\n",
    "N=100\n",
    "x = t.randn(N, N, device=device)\n",
    "w = t.randn(N, N, device=device)\n",
    "_, _ = squantization_matmul_stats(x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQuantizedLinear(nn.Module):\n",
    "    def __init__(self, linear):\n",
    "        super().__init__()\n",
    "        w, scale = squantize(linear.weight.T)\n",
    "        self.w = w\n",
    "        self.register_buffer('w_matrix', self.w)\n",
    "        self.scale_w = scale\n",
    "        self.bias = linear.bias # keep bias unquantized\n",
    "\n",
    "    def forward(self, x):\n",
    "        xq, scale_x = squantize(x)\n",
    "        yq = matmul_i8i32(xq, self.w)\n",
    "        scale_y = self.scale_w * scale_x\n",
    "        y = sunquantize(yq, scale_y)\n",
    "        y = y + self.bias\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base fp32: 0.9468\n",
      "quantized int8: 0.9464\n"
     ]
    }
   ],
   "source": [
    "def squantize_module(module):\n",
    "    for name, node in module.named_children():\n",
    "        if isinstance(node, nn.Linear):\n",
    "            setattr(module, name, SQuantizedLinear(node))\n",
    "        else:\n",
    "            squantize_module(node)\n",
    "\n",
    "weights = t.load('weights/mnist-1024-128-64-10_2024-08-12_00h35.pt')\n",
    "mnist_base = Mnist().to(device)\n",
    "mnist_base.load_state_dict(weights)\n",
    "mnist = Mnist().to(device)\n",
    "mnist.load_state_dict(weights)\n",
    "squantize_module(mnist)\n",
    "\n",
    "print(f'base fp32: {eval(mnist_base)[1]}')\n",
    "print(f'quantized int8: {eval(mnist)[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base fp32 size: 560424\n",
      "quantized int8 size: 140712\n",
      "quantize / base ratio: 0.25\n"
     ]
    }
   ],
   "source": [
    "def model_size(model):\n",
    "    return sum(p.numel() * p.element_size() for p in model.parameters()) + \\\n",
    "        sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "\n",
    "print(f'base fp32 size: {model_size(mnist_base)}')\n",
    "print(f'quantized int8 size: {model_size(mnist)}')\n",
    "print(f'quantize / base ratio: {model_size(mnist) / model_size(mnist_base):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## asymmetric quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(weights, bits=8):\n",
    "    ''' using the min-max strategy, this is vulnerable to outliers '''\n",
    "    assert bits <= 8 # keep my life simple\n",
    "    maxi = weights.max()\n",
    "    mini = weights.min()\n",
    "    qmaxi = 2 ** bits - 1\n",
    "    scale = (maxi - mini) / qmaxi\n",
    "    zero = int(t.round(-mini / scale))\n",
    "    quantized_weights = t.clamp(t.round(weights / scale) + zero, 0, qmaxi).to(t.uint8)\n",
    "    return quantized_weights, scale, zero\n",
    "\n",
    "def unquantize(quantized_weights, scale, zero):\n",
    "    quantized_weights = quantized_weights.to(t.int32)\n",
    "    return (quantized_weights - zero) * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bounds(weights, bits=8):\n",
    "    qw, _, _ = quantize(weights, bits=bits)\n",
    "    assert qw.min() == 0\n",
    "    assert qw.max() == 2**bits - 1\n",
    "\n",
    "def test_round_trip(weights, bits=8, threshold=0.02):\n",
    "    ''' this is flaky at best '''\n",
    "    qw, scale, zero = quantize(weights, bits=bits)\n",
    "    w = unquantize(qw, scale, zero)\n",
    "    assert weights.allclose(w, atol=threshold), f'{(weights - w).abs().max()=}'\n",
    "\n",
    "def test_more_bits_do_better(weights):\n",
    "    ''' flaky too '''\n",
    "    def encode_decode_max_abs_error(bits):\n",
    "        qw, scale, zero = quantize(weights, bits=bits)\n",
    "        w = unquantize(qw, scale, zero)\n",
    "        return (weights - w).abs().max()\n",
    "    error1 = encode_decode_max_abs_error(bits=4)\n",
    "    error2 = encode_decode_max_abs_error(bits=6)\n",
    "    error3 = encode_decode_max_abs_error(bits=8)\n",
    "    assert error1 > error2 > error3, f'{error1=}, {error2=}, {error3=}'\n",
    "\n",
    "for _ in range(100):\n",
    "    xs = t.randn(100, 100)\n",
    "    test_bounds(xs)\n",
    "    test_round_trip(xs)\n",
    "    test_more_bits_do_better(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.49838921427726746 0.498381644487381\n",
      "max: 0.9999037981033325 0.9997287392616272\n",
      "min: 0.00017511844635009766 0.0\n",
      "atol: 0.0019600391387939453\n",
      "mse: 1.2681715588769293e-06\n"
     ]
    }
   ],
   "source": [
    "def quantization_stats(weights):\n",
    "    qw, scale, zero = quantize(weights, bits=8)\n",
    "    w = unquantize(qw, scale, zero)\n",
    "    print(f'mean: {weights.mean()} {w.mean()}')\n",
    "    print(f'max: {weights.max()} {w.max()}')\n",
    "    print(f'min: {weights.min()} {w.min()}')\n",
    "    print(f'atol: {(weights - w).abs().max()}')\n",
    "    print(f'mse: {(weights - w).pow(2).mean()}')\n",
    "\n",
    "xs = t.rand(10000)\n",
    "quantization_stats(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "x &= (x_{quant} - zero_x) * scale_x \\\\\n",
    "w &= (w_{quant} - zero_w) * scale_w \\\\\n",
    "y &= x @ w \\\\\n",
    "  &= ((x_{quant} - zero_x) * scale_x) @ ((w_{quant} - zero_w) * scale_w) \\\\\n",
    "  &= scale_x * scale_w * (x_{quant} - zero_x) @ (w_{quant} - zero_w) \\\\\n",
    "  &= scale_x * scale_w * (x_{quant} @ w_{quant} - x_{quant} @ zero_w - zero_x @ w_{quant} + zero_x @ zero_w) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_uint8_int32_matmul(x, w):\n",
    "    ''' pretend we have a uint8 matmul with int32 accumulator '''\n",
    "    x = x.to(t.int32)\n",
    "    w = w.to(t.int32)\n",
    "    assert (x >= 0).all()\n",
    "    assert (w >= 0).all()\n",
    "    y = t.matmul(x, w)\n",
    "    assert (y >= 0).all()\n",
    "    return y\n",
    "\n",
    "def asymmetric_quant_matmul(xq, wq, scale_x, scale_w, zero_x, zero_w):\n",
    "    '''\n",
    "    NOTE: This not efficient, and defeat the purpose of quantization.\n",
    "    I just want to test the correctness of the formula.\n",
    "    Original formula lives in https://arxiv.org/pdf/2106.08295 equation (13)\n",
    "    '''\n",
    "    # this kills the purpose of quantization.\n",
    "    xq = xq.to(t.int32)\n",
    "    wq = wq.to(t.int32)\n",
    "\n",
    "    # several tricks here, the .sum() make the dimensions match\n",
    "    # and note the xq.shape[1] here that doesn't appear in the paper!!\n",
    "    unscaled_y = (\n",
    "        fake_uint8_int32_matmul(xq, wq)\n",
    "        - xq.sum(1, keepdim=True) * zero_w\n",
    "        - zero_x * wq.sum(0, keepdim=True)\n",
    "        + xq.shape[1] * zero_x * zero_w) # here\n",
    "    y = scale_x * scale_w * unscaled_y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.017011694610118866 0.01569465361535549\n",
      "max: 43.04981994628906 43.039249420166016\n",
      "min: -47.28977966308594 -47.233360290527344\n",
      "atol: 0.49615478515625\n",
      "mse: 0.017323683947324753\n"
     ]
    }
   ],
   "source": [
    "def quantization_matmul_stats(x, w):\n",
    "    y = x @ w\n",
    "    xq, scale_x, zero_x = quantize(x)\n",
    "    wq, scale_w, zero_w = quantize(w)\n",
    "    yy = asymmetric_quant_matmul(xq, wq, scale_x, scale_w, zero_x, zero_w)\n",
    "\n",
    "    print(f'mean: {y.mean()} {yy.mean()}')\n",
    "    print(f'max: {y.max()} {yy.max()}')\n",
    "    print(f'min: {y.min()} {yy.min()}')\n",
    "    print(f'atol: {(y - yy).abs().max()}')\n",
    "    print(f'mse: {(y - yy).pow(2).mean()}')\n",
    "    return y, yy\n",
    "\n",
    "N=100\n",
    "x = t.randn(N, N)\n",
    "w = t.randn(N, N)\n",
    "_, _ = quantization_matmul_stats(x, w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
