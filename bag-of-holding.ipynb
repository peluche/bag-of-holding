{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bag of holding (aka. making neural networks smaller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import t as t\n",
    "import t.nn as nn\n",
    "import t.optim as optim\n",
    "import t.nn.functional as F\n",
    "from t.utils.data import DataLoader\n",
    "from tvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "device = 'cuda' if t.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 640\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    if img.shape[0] == 1: img = img.permute(1, 2, 0)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(train_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(784, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 10),\n",
    "            # nn.Linear(784, 32),\n",
    "            # nn.LeakyReLU(),\n",
    "            # nn.Linear(32, 32),\n",
    "            # nn.LeakyReLU(),\n",
    "            # nn.Linear(32, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x.flatten(start_dim=-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.no_grad()\n",
    "def eval(model, dataloader=test_loader):\n",
    "    model.eval()\n",
    "    loss, accuracy, count = 0, 0, 0\n",
    "    for x, y in dataloader:\n",
    "        logits = model(x.to(device))\n",
    "        loss += F.cross_entropy(logits, y.to(device)).item()\n",
    "        accuracy += (logits.argmax(1) == y.to(device)).sum().item()\n",
    "        count += len(x)\n",
    "    model.train()\n",
    "    return loss / count, accuracy / count\n",
    "\n",
    "def eval_dict(model, dataloader=test_loader, name='test'):\n",
    "    loss, accuracy = eval(model, dataloader=dataloader)\n",
    "    return {f'{name}_loss': loss, f'{name}_accuracy': accuracy}\n",
    "\n",
    "# eval_dict(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, opt, dataloader=train_loader, wnb=True, epochs=1000):\n",
    "    model.train()\n",
    "    if wnb: wandb.init()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for x, y in dataloader:\n",
    "            logits = model(x.to(device))\n",
    "            loss = F.cross_entropy(logits, y.to(device))\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            if wnb: wandb.log({'epoch': epoch} | eval_dict(model) | eval_dict(model, dataloader=train_loader, name='train'))\n",
    "    if wnb: wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/p/ml/bag-of-holding/wandb/run-20240810_221536-nwrc37tn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peluche/bag-of-holding/runs/nwrc37tn' target=\"_blank\">effortless-thunder-9</a></strong> to <a href='https://wandb.ai/peluche/bag-of-holding' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peluche/bag-of-holding' target=\"_blank\">https://wandb.ai/peluche/bag-of-holding</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peluche/bag-of-holding/runs/nwrc37tn' target=\"_blank\">https://wandb.ai/peluche/bag-of-holding/runs/nwrc37tn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [08:07<00:00,  4.88s/it]\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e823529d75bb48d8a08a53f93a72e502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.009 MB uploaded\\r'), FloatProgress(value=0.2614125753660637, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>test_accuracy</td><td>▁▆▇████████</td></tr><tr><td>test_loss</td><td>█▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▆▇▇▇██████</td></tr><tr><td>train_loss</td><td>█▃▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>test_accuracy</td><td>0.9792</td></tr><tr><td>test_loss</td><td>0.00016</td></tr><tr><td>train_accuracy</td><td>0.99992</td></tr><tr><td>train_loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">effortless-thunder-9</strong> at: <a href='https://wandb.ai/peluche/bag-of-holding/runs/nwrc37tn' target=\"_blank\">https://wandb.ai/peluche/bag-of-holding/runs/nwrc37tn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240810_221536-nwrc37tn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist = Mnist().to(device)\n",
    "opt = optim.AdamW(mnist.parameters(), lr=3e-4)\n",
    "train(mnist, opt, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(mnist.state_dict(), f'weights/mnist-128-64-10_{datetime.now().strftime(\"%Y-%m-%d_%Hh%M\")}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "t quantize(weights, bits=8):\n",
    "    ''' using the min-max strategy, this is vulnerable to outliers '''\n",
    "    maxi = weights.max()\n",
    "    mini = weights.min()\n",
    "    qmaxi = 2 ** bits - 1\n",
    "    scale = (maxi - mini) / qmaxi\n",
    "    zero = int(t.round(-mini / scale))\n",
    "    quantized_weights = t.clamp(t.round(weights / scale) + zero, 0, qmaxi)\n",
    "    return quantized_weights, scale, zero\n",
    "\n",
    "def unquantize(quantized_weights, scale, zero):\n",
    "    return (quantized_weights - zero) * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bounds(weights, bits=8):\n",
    "    qw, _, _ = quantize(weights, bits=bits)\n",
    "    assert qw.min() == 0\n",
    "    assert qw.max() == 2**bits - 1\n",
    "\n",
    "def test_round_trip(weights, bits=8, threshold=0.02):\n",
    "    ''' this is flaky at best '''\n",
    "    qw, scale, zero = quantize(weights, bits=bits)\n",
    "    w = unquantize(qw, scale, zero)\n",
    "    assert weights.allclose(w, atol=threshold), f'{(weights - w).abs().max()=}'\n",
    "\n",
    "def test_more_bits_do_better(weights):\n",
    "    ''' flaky too '''\n",
    "    def encode_decode_max_abs_error(bits):\n",
    "        qw, scale, zero = quantize(weights, bits=bits)\n",
    "        w = unquantize(qw, scale, zero)\n",
    "        return (weights - w).abs().max()\n",
    "    error1 = encode_decode_max_abs_error(bits=4)\n",
    "    error2 = encode_decode_max_abs_error(bits=8)\n",
    "    error3 = encode_decode_max_abs_error(bits=16)\n",
    "    assert error1 > error2 > error3, f'{error1=}, {error2=}, {error3=}'\n",
    "\n",
    "for _ in range(100):\n",
    "    xs = t.randn(100, 100)\n",
    "    test_bounds(xs)\n",
    "    test_round_trip(xs)\n",
    "    test_more_bits_do_better(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5032405853271484 0.5032347440719604\n",
      "max: 0.9999611377716064 0.9999443888664246\n",
      "min: 1.6748905181884766e-05 0.0\n",
      "atol: 0.0019606947898864746\n",
      "mse: 1.2573027561302297e-06\n"
     ]
    }
   ],
   "source": [
    "def quantization_stats(weights):\n",
    "    qw, scale, zero = quantize(weights, bits=8)\n",
    "    w = unquantize(qw, scale, zero)\n",
    "    print(f'mean: {weights.mean()} {w.mean()}')\n",
    "    print(f'max: {weights.max()} {w.max()}')\n",
    "    print(f'min: {weights.min()} {w.min()}')\n",
    "    print(f'atol: {(weights - w).abs().max()}')\n",
    "    print(f'mse: {(weights - w).pow(2).mean()}')\n",
    "\n",
    "xs = t.rand(10000)\n",
    "quantization_stats(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedLinear(nn.Module):\n",
    "    def __init__(self, linear):\n",
    "        super().__init__()\n",
    "        w, scale, zero = quantize(linear.weight)\n",
    "        self.w = nn.Parameter(w.to(t.uint8))\n",
    "        self.scale = scale\n",
    "        self.zero = zero\n",
    "        self.bias = linear.bias # keep bias unquantized\n",
    "        # self.running_scale = None\n",
    "        # self.running_zero = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # if self.running_scale is None:\n",
    "        #     self.running_scale = x.\n",
    "        xq, scale, zero = quantize(x)\n",
    "        xq = xq.to(t.uint8)\n",
    "        yq = xq @ self.w\n",
    "        y = unquantize(yq, scale, zero)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear: 0 Linear(in_features=784, out_features=128, bias=True)\n",
      "getattr(module, name)=Linear(in_features=784, out_features=128, bias=True)\n",
      "linear: 2 Linear(in_features=128, out_features=64, bias=True)\n",
      "getattr(module, name)=Linear(in_features=128, out_features=64, bias=True)\n",
      "linear: 4 Linear(in_features=64, out_features=10, bias=True)\n",
      "getattr(module, name)=Linear(in_features=64, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "def quantize(module):\n",
    "    for name, node in module.named_children():\n",
    "        if isinstance(node, nn.Linear):\n",
    "            print('linear:', name, node)\n",
    "            print(f'{getattr(module, name)=}')\n",
    "        else:\n",
    "            quantize(node)\n",
    "\n",
    "quantize(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atol: 0.0\n",
      "diff > 1e-5: 0 / 1048576\n",
      "allclose: True\n"
     ]
    }
   ],
   "source": [
    "# __heavily__ inspired by https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html\n",
    "\n",
    "@triton.jit\n",
    "def matmul_kernel(\n",
    "        # Pointers to matrices\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        # Matrix dimensions\n",
    "        M, N, K,\n",
    "        # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "        # by to get the element one row down (A has M rows).\n",
    "        stride_am, stride_ak,\n",
    "        stride_bk, stride_bn,\n",
    "        stride_cm, stride_cn,\n",
    "        # Meta-parameters\n",
    "        BLOCK_SIZE_M: tl.constexpr,\n",
    "        BLOCK_SIZE_N: tl.constexpr,\n",
    "        BLOCK_SIZE_K: tl.constexpr,\n",
    "        GROUP_SIZE_M: tl.constexpr,\n",
    "):\n",
    "    ''' Kernel for computing the matmul C = A x B.\n",
    "    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n",
    "    '''\n",
    "    # -----------------------------------------------------------\n",
    "    # Map program ids `pid` to the block of C it should compute.\n",
    "    # This is done in a grouped ordering to promote L2 data reuse.\n",
    "    # See above `L2 Cache Optimizations` section for details.\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Create pointers for the first blocks of A and B.\n",
    "    # We will advance this pointer as we move in the K direction\n",
    "    # and accumulate\n",
    "    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n",
    "    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n",
    "    # See above `Pointer Arithmetic` section for details\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator = tl.dot(a, b, accumulator)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "    c = accumulator.to(tl.int32)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "def matmul_i8i32(a, b):\n",
    "    ''' matmul for int8 with int32 accumulators '''\n",
    "    assert a.shape[1] == b.shape[0], 'incompatible dimensions'\n",
    "    assert a.is_contiguous(), 'matrix A must be contiguous'\n",
    "    M, K = a.shape\n",
    "    K, N = b.shape\n",
    "    assert M % 32 == 0 and N % 32 == 0 and K % 32 == 0, 'matrices must be divisible by 32'\n",
    "    c = t.empty((M, N), device=a.device, dtype=t.int32)\n",
    "    # launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    matmul_kernel[grid](\n",
    "        a, b, c,\n",
    "        M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c.stride(0), c.stride(1),\n",
    "        BLOCK_SIZE_M=32,\n",
    "        BLOCK_SIZE_N=32,\n",
    "        BLOCK_SIZE_K=32,\n",
    "        GROUP_SIZE_M=8,\n",
    "    )\n",
    "    return c\n",
    "\n",
    "a = t.randint(-128, 128, (512, 1024), device='cuda', dtype=t.int8)\n",
    "b = t.randint(-128, 128, (1024, 2048), device='cuda', dtype=t.int8)\n",
    "triton_output = matmul_i8i32(a, b)\n",
    "torch_output = t.matmul(a.to(t.float32), b.to(t.float32))\n",
    "\n",
    "print(f'atol: {t.max(t.abs(triton_output - torch_output))}')\n",
    "print(f'diff > 1e-5: {t.sum(t.abs(triton_output - torch_output) > 1e-5)} / {triton_output.numel()}')\n",
    "print(f'allclose: {triton_output.float().allclose(torch_output)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check that int8 and uint8 behave the same\n",
    "# because torch does not support uint8 on GPU\n",
    "a = t.randint(-128, 128, (512,), device='cuda', dtype=t.int8)\n",
    "b = t.randint(-128, 128, (512,), device='cuda', dtype=t.int8)\n",
    "signed_sum = a + b\n",
    "unsigned_sum = a.to(t.uint8) + b.to(t.uint8)\n",
    "\n",
    "assert signed_sum.to(t.uint8).allclose(unsigned_sum)\n",
    "assert signed_sum.allclose(unsigned_sum.to(t.int8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
